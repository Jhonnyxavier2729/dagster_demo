#  ETL Automatizado para An√°lisis de Datos üöÄ

Este proyecto implementa un pipeline ETL (Extracci√≥n, Transformaci√≥n y Carga) robusto y automatizado utilizando **Dagster**. El objetivo principal es ingestar datos desde [Menciona la fuente, ej: una API de ventas, archivos CSV, una base de datos], procesarlos para garantizar su calidad y estructura, y finalmente cargarlos en un destino [Menciona el destino, ej: un Data Warehouse como BigQuery, PostgreSQL] para su posterior an√°lisis y visualizaci√≥n.

[<img width="2048" height="1556" alt="image" src="https://github.com/user-attachments/assets/9a336fea-ba36-4761-9b3f-edbebf1cf670" />]

---

## √çndice

- [Acerca del Proyecto](#acerca-del-proyecto-)
- [Caracter√≠sticas Principales](#caracter√≠sticas-principales-)
- [Stack Tecnol√≥gico](#stack-tecnol√≥gico-Ô∏è)
- [Estructura del Proyecto](#estructura-del-proyecto-)
- [C√≥mo Empezar](#c√≥mo-empezar-)
- [Uso del Pipeline](#uso-del-pipeline-Ô∏è)
- [Automatizaci√≥n](#automatizaci√≥n-‚è∞)


---

## Acerca del Proyecto üìä

En el mundo del an√°lisis de datos, la calidad y disponibilidad de la informaci√≥n es crucial. Este proyecto aborda el desaf√≠o de recolectar datos crudos de diversas fuentes y transformarlos en un formato limpio, confiable y listo para ser analizado.

Utilizamos **Dagster** como orquestador central debido a su enfoque en el desarrollo local, la observabilidad y la declaraci√≥n de dependencias expl√≠citas entre los pasos del ETL. Esto nos permite construir pipelines de datos fiables, mantenibles y f√°ciles de depurar.

El pipeline realiza las siguientes acciones:
1.  **Extracci√≥n**: Conecta con [Fuente de datos] y extrae la informaci√≥n m√°s reciente.
2.  **Transformaci√≥n**: Limpia los datos, enriquece la informaci√≥n, aplica reglas de negocio y los modela para el an√°lisis.
3.  **Carga**: Inserta los datos procesados en [Destino de datos], listos para ser consumidos por herramientas de BI o notebooks de an√°lisis.

---

## Caracter√≠sticas Principales ‚ú®

- **Orquestaci√≥n Moderna**: Todo el flujo de trabajo es gestionado por Dagster, permitiendo visualizar el grafo de dependencias (DAG), monitorear ejecuciones y gestionar el estado.
- **Automatizaci√≥n**: El pipeline est√° programado para ejecutarse autom√°ticamente, asegurando que los datos est√©n siempre actualizados.
- **Calidad de Datos**: Se implementan validaciones en los pasos de transformaci√≥n para garantizar la integridad y consistencia de los datos.
- **Modularidad**: El c√≥digo est√° organizado en "activos" (assets) de Dagster, lo que facilita la adici√≥n de nuevos pasos o la modificaci√≥n de los existentes.
- **Entorno de Desarrollo Local**: Dagster permite ejecutar y probar todo el pipeline en un entorno local antes de desplegarlo a producci√≥n.

---

## Stack Tecnol√≥gico ‚öôÔ∏è

- **Lenguaje**: Python 3.9+
- **Orquestador ETL**: Dagster
- **Procesamiento de Datos**: Pandas / Polars 
- **Base de Datos / Data Warehouse**: Duckbd
- **Gestor de Dependencias**: pip / uv

---

